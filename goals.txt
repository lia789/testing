



Application Description:
"""
Build a web scraping application about quotes scraping based on user tag input from "https://www.goodreads.com/quotes/tag/love"

Application core feature:
	1. User input tag name
	2. Scraping data based on tag name
	3. Display Scrapy spider status
	4. Download quotes data as CSV file
"""


Goals of the project:
	1. Web application design document building and supported files
	2. Django cheat sheet
	3. Web scraping cheat sheet
	4. Building a architecture about Scrapy, Django, ScrapyD and JavaScript





I have a web scraping application project based on Django, Scrapy, and Scrapyd
User will input a keyword from home page, then keyword will pass to Scrapy for scraping data based on keyword.
Scrapy take 10 minute for finish data scraping.
After finish scraping data, user will download data as CSV file.
There will be a result page that will display Scrapy spider current status.
Once spider start user can close the browser and after some time propably after 15 to 20 minute,
user back again and download the data.

What will be best architecture about this application?

Does I need use session, celery, or something else? How to display Scrapy spider running status?
Give me best project development guide and suggested tools and steps.

I have a project web scraping application project. User will input a keyword, that keyword will use scraping data. My scraping project based on Scrapy framewrk. After finish scraping data, user will download





I have 3 field:
author_name: "Albart"
number_of_likes: 10
quotes_text: "The question of whether a computer can think is no more interesting than the question of whether a submarine can swim."


"""
import re
from itemadapter import ItemAdapter

class GoodReaderProjectPipeline:
    def process_item(self, item, spider):
        return item

class DataCleanerPipeline(GoodReaderProjectPipeline):
    def process_item(self, item, spider):
        item["quotes_text"] = item["quotes_text"][1:-1]
        item["number_of_likes"] = int(re.findall(r"\d+", item["number_of_likes"])[0]) if item.get("number_of_likes") else 0
        item["author_name"] = item["author_name"][:-1]
        return item
"""
This is my spider pipeline code
My django database name is "db.sqlite3"

Write code that save scraping data on "db.sqlite3" data base









"""
# Define your item pipelines here
#
# Don't forget to add your pipeline to the ITEM_PIPELINES setting
# See: https://docs.scrapy.org/en/latest/topics/item-pipeline.html


# useful for handling different item types with a single interface

import sqlite3
from itemadapter import ItemAdapter



# class ZillowPipeline(object):
#     def process_item(self, item, spider):
#         return item

class ZillowPipeline:

    def __init__(self, sqlite_file):
        self.sqlite_file = sqlite_file

    @classmethod
    def from_crawler(cls, crawler):
        return cls(
            sqlite_file=crawler.settings.get('SQLITE_FILE')
        )

    def open_spider(self, spider):
        self.conn = sqlite3.connect(self.sqlite_file)
        self.cur = self.conn.cursor()
        self.cur.execute("""CREATE TABLE IF NOT EXISTS zillow_app_zillowproperty (
                            property_url TEXT PRIMARY KEY,
                            zip_id TEXT,
                            img_url TEXT,
                            zip_code TEXT,
                            city TEXT,
                            state TEXT,
                            latitude REAL,
                            longitude REAL,
                            price REAL,
                            bath_room INTEGER,
                            bed_room INTEGER,
                            home_type TEXT,
                            days_on_zillow INTEGER,
                            rent_zestimate INTEGER,
                            lot_area_value REAL
                        )""")

    def close_spider(self, spider):
        self.conn.commit()
        self.conn.close()

    def process_item(self, item, spider):
        self.cur.execute("SELECT property_url FROM zillow_app_zillowproperty WHERE property_url=?", (item['property_url'],))
        result = self.cur.fetchone()
        if result:
            spider.logger.warn("Item already in database: %s" % item['property_url'])
            return item
        else:
            self.cur.execute("""INSERT INTO zillow_app_zillowproperty (
                                property_url, zip_id, img_url, zip_code, city, state,
                                latitude, longitude, price, bath_room, bed_room, home_type,
                                days_on_zillow, rent_zestimate, lot_area_value
                            ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)""",
                            (item['property_url'], item['zip_id'], item['img_url'], item['zip_code'], item['city'], item['state'],
                             item['latitude'], item['longitude'], item['price'], item['bath_room'], item['bed_room'], item['home_type'],
                             item['days_on_zillow'], item['rent_zestimate'], item['lot_area_value']))
            return item


class DataCleanerPipeline(ZillowPipeline):
    def process_item(self, item, spider):  # Define columns, that need data cleaning
        item["bath_room"] = int(item["bath_room"]) if item["bath_room"] is not None else None
        item["bed_room"] = int(item["bed_room"]) if item["bed_room"] is not None else None
        item["price"] = int(item["price"]) if item["price"] is not None else None
        item["days_on_zillow"] = abs(item.get("days_on_zillow", 0) or 0)
        item["property_url"] = f"https://www.zillow.com{item['property_url']}"

        return item

"""


This is my old Scrapy code, that save data on database,

"""
import re
from itemadapter import ItemAdapter
from tags_app.models import Quote

class GoodReaderProjectPipeline:
    def process_item(self, item, spider):
        # Create a new Quote object with the cleaned data

        return item

class DataCleanerPipeline(GoodReaderProjectPipeline):
    def process_item(self, item, spider):
        item["quotes_text"] = item["quotes_text"][1:-1]
        item["number_of_likes"] = int(re.findall(r"\d+", item["number_of_likes"])[0]) if item.get("number_of_likes") else 0
        item["author_name"] = item["author_name"][:-1]

        Quote.objects.create(
            author_name=item['author_name'],
            number_of_likes=item['number_of_likes'],
            quotes_text=item['quotes_text']
        )

        return item
"""

This is my current project code, write database code similar like my old project
