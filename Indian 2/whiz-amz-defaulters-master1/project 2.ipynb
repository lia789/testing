{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f35e11-a7d4-4987-9f60-1e21606f8384",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from whizzbox import amazon_sites, db_connector, toolkit, config, s3_connector as s3c, gsheet_connector as gc\n",
    "from whizzbox import custom_errors\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4fb4901-74f7-4466-a324-5b1c49beb894",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_NAME = 'whiz-amz-defaulters'\n",
    "SAMPLE = True  # if it's a sample, no. of sites are limited\n",
    "SEND_FAIL_EMAIL = True\n",
    "UPDATE_GSHEET = False\n",
    "UPLOAD_TO_S3 = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa325fdc-371b-4d06-bb17-644cf557f534",
   "metadata": {},
   "outputs": [],
   "source": [
    "# noinspection SpellCheckingInspection\n",
    "def get_service_area_id():\n",
    "    url = \"https://logistics.amazon.in/flex/api/getOperationalRegions\"\n",
    "\n",
    "    payload = \"\"\n",
    "    headers = {\"cookie\": \"session-id=260-6428466-7223316; session-id-time=2082787201l; \"\n",
    "                         \"session-token=GbElPtRlx37WtbcOToTC3cRl2schxCQbGfw9PermyrbyTyonOdmMyDvN9xAn3Y1u\"\n",
    "                         \"urDFHnNpykoR0H9l8R%2Bw0pqKOD14Zwtk4QClUVQpQsIItI6ETeBDN0M5SpFCUku0BGvpR2xSOFJN\"\n",
    "                         \"nMUMDfYiflLntxjcLhKYtat1HekaJ2LZEwTlJ3dL6%2Fs%2B9U37Zu8JIQq%2FK7qPG5HFr4SwvqwIoQ\"\n",
    "                         \"pyvIMoFI7ZbtIDa1gNSij8nXYZmqmACRQ0FUVbIFHYlcHjlwmkXzg%3D\"}\n",
    "\n",
    "    response = requests.request(\"GET\", url, data=payload, headers=headers)\n",
    "\n",
    "    data = json.loads(response.text)\n",
    "    req_data = data\n",
    "    api_df_raw = pd.DataFrame(req_data)\n",
    "    api_df = api_df_raw.explode('basicServiceAreas')\n",
    "    api_df_inter = pd.concat([api_df.drop(['basicServiceAreas'], axis=1),\n",
    "                              api_df['basicServiceAreas'].apply(pd.Series)], axis=1)\n",
    "\n",
    "    df_final = pd.concat([api_df_inter.drop(['pickUpLocationAddress', 'pickUpLocation'], axis=1),\n",
    "                          api_df_inter['pickUpLocationAddress'].apply(pd.Series),\n",
    "                          api_df_inter['pickUpLocation'].apply(pd.Series)], axis=1)\n",
    "\n",
    "    final_df = df_final[['defaultStationCode', 'serviceAreaID', 'regionID', 'regionName',\n",
    "                         'state', 'postalCode', 'longitude', 'latitude', 'active']]\n",
    "    final_df.columns = ['site_code', 'service_area_id', 'region_id', 'region_name',\n",
    "                        'state', 'postal_code', 'longitude', 'latitude', 'active']\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac32f13-06f6-44b8-a4fd-b3709270902c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# noinspection SpellCheckingInspection\n",
    "def get_drivers_data(yyyy_mm_dd, service_area_id):\n",
    "    url = \"https://logistics.amazon.in/operations/execution/api/summaries\"\n",
    "\n",
    "    querystring = {\"localDate\": f\"{yyyy_mm_dd}\", \"serviceAreaId\": f\"{service_area_id}\"}\n",
    "\n",
    "    payload = \"\"\n",
    "    headers = {\n",
    "        \"cookie\": 'session-id=258-5953668-1245245; ubid-acbin=261-0863908-3910552; '\n",
    "                  'lc-acbin=en_IN; x-acbin=\"57E9eKz@7A@To6Vddbaqer5jgdCIqlKfP2T8kM6wKCICy?gDv7vle?RKDpKPgjZK\";'\n",
    "                  ' at-acbin=Atza|IwEBIJk3W45PoKDQRfBKht6R7k4X7GddLPM54_esWjs8dHrH6815nPGyW5DMuqexf1w0bDDz10cOG'\n",
    "                  'JJEgdOwPR3Us-wmX2ChldzceR7pz7ciQRrZdKYRDV5A-L0kRQRtlbXvJvefFPPYLQUohVZ59kgsrTlSnAMO2MUVSxI8q'\n",
    "                  'sBOXW2fPqGjf3o3IIx_f3T8ASPkGXUQcS2C9yTpnncFHoyCgAcCfs0dyQBBJzFYedq3XUQLVg; sess-at-acbin'\n",
    "                  '=\"ckW6tTbMs1yigrh3KsKaTVeibCPC2YHIFdC5PnL4F+Q=\"; x-amz-log-portal-locale=en-IN; session'\n",
    "                  '-id-time=2082787201l; session-token=F8JjOaEew4Id8WCibAXIjaVT8YkIKwPtKpnJQf0jBIrqTbaLb4Y2Kv47'\n",
    "                  'nJXwvdCTKygovIhu+XF4jOcoVSZ7ImRYrmDOYEUTdlQ6uYK2r9jmnmHdDBLkl/HGcgDFFKrrJidRt61RI4MJ1PGgzhW'\n",
    "                  'OIWRMV7OAW+QVoHlg45HkGmhxM9AGo2+x2Ef52i5bwyjCBkSruiVfXAjKD+lMC/jz+1xLh2P9+ZIGbPtnFScrRdjrq'\n",
    "                  'FJEehq9CQ',\n",
    "        \"authority\": \"logistics.amazon.in\",\n",
    "        \"accept\": \"application/json, text/plain, */*\",\n",
    "        \"accept-language\": \"en-GB,en-US;q=0.9,en;q=0.8\",\n",
    "        \"referer\": f\"https://logistics.amazon.in/operations/execution/itineraries?selectedDay={yyyy_mm_dd}\"\n",
    "                   f\"&serviceAreaId={service_area_id}\",\n",
    "        \"sec-ch-ua\": '\"Not_A Brand\";v=\"99\", \"Google Chrome\";v=\"109\", \"Chromium\";v=\"109\"',\n",
    "        \"sec-ch-ua-mobile\": \"?0\",\n",
    "        \"sec-ch-ua-platform\": '\"Windows\"',\n",
    "        \"sec-fetch-dest\": \"empty\",\n",
    "        \"sec-fetch-mode\": \"cors\",\n",
    "        \"sec-fetch-site\": \"same-origin\",\n",
    "        \"user-agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "                      \"Chrome/109.0.0.0 Safari/537.36\"\n",
    "    }\n",
    "\n",
    "    response = requests.request(\"GET\", url, data=payload, headers=headers, params=querystring)\n",
    "    data = json.loads(response.text)\n",
    "    driver_df = pd.DataFrame(data['transporters'])\n",
    "    if not driver_df.empty:\n",
    "        driver_ids = list(driver_df['transporterId'].unique())\n",
    "        itinerary_df = pd.DataFrame(data['itinerarySummaries'])\n",
    "        packages_df = pd.DataFrame(data['transporterPackageSummaries']) if data['transporterPackageSummaries'] \\\n",
    "            else pd.DataFrame()\n",
    "        loaction_df = get_drivers_location(yyyy_mm_dd, service_area_id, driver_ids)\n",
    "        merged_df = pd.merge(itinerary_df, pd.merge(driver_df, loaction_df, on='transporterId', how='left'),\n",
    "                             on='transporterId', how='left')\n",
    "        merged_df['driver_name'] = merged_df['firstName'].str.title() + ' ' + merged_df['lastName'].str.title()\n",
    "        if not packages_df.empty:\n",
    "            merged_df_final = pd.merge(merged_df, packages_df[['transporterId', 'packageStatus']],\n",
    "                                       on='transporterId', how='left')\n",
    "        else:\n",
    "            merged_df_final = merged_df.copy()\n",
    "        df_final = merged_df_final.copy()\n",
    "    else:\n",
    "        df_final = pd.DataFrame()\n",
    "    return df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec806df-16ca-4572-9d14-8a224ff37afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# noinspection SpellCheckingInspection\n",
    "\n",
    "def get_drivers_location(yyyy_mm_dd, service_area_id, transporter_ids: list):\n",
    "    url = \"https://logistics.amazon.in/operations/execution/api/transporters/locationUpdate\"\n",
    "    payload = {\n",
    "        \"transporterIds\": transporter_ids}\n",
    "    headers = {\n",
    "        \"authority\": \"logistics.amazon.in\",\n",
    "        \"accept\": \"application/json, text/plain, */*\",\n",
    "        \"accept-language\": \"en-GB,en-US;q=0.9,en;q=0.8\",\n",
    "        \"content-type\": \"application/json;charset=UTF-8\",\n",
    "        \"cookie\": 'session-id=258-5953668-1245245; ubid-acbin=261-0863908-3910552; '\n",
    "                  'lc-acbin=en_IN; x-acbin=\"57E9eKz@7A@To6Vddbaqer5jgdCIqlKfP2T8kM6wKCICy?gDv7vle?RKDpKPgjZK\";'\n",
    "                  ' at-acbin=Atza|IwEBIJk3W45PoKDQRfBKht6R7k4X7GddLPM54_esWjs8dHrH6815nPGyW5DMuqexf1w0bDDz10cOG'\n",
    "                  'JJEgdOwPR3Us-wmX2ChldzceR7pz7ciQRrZdKYRDV5A-L0kRQRtlbXvJvefFPPYLQUohVZ59kgsrTlSnAMO2MUVSxI8q'\n",
    "                  'sBOXW2fPqGjf3o3IIx_f3T8ASPkGXUQcS2C9yTpnncFHoyCgAcCfs0dyQBBJzFYedq3XUQLVg; sess-at-acbin'\n",
    "                  '=\"ckW6tTbMs1yigrh3KsKaTVeibCPC2YHIFdC5PnL4F+Q=\"; x-amz-log-portal-locale=en-IN; session'\n",
    "                  '-id-time=2082787201l; session-token=F8JjOaEew4Id8WCibAXIjaVT8YkIKwPtKpnJQf0jBIrqTbaLb4Y2Kv47'\n",
    "                  'nJXwvdCTKygovIhu+XF4jOcoVSZ7ImRYrmDOYEUTdlQ6uYK2r9jmnmHdDBLkl/HGcgDFFKrrJidRt61RI4MJ1PGgzhW'\n",
    "                  'OIWRMV7OAW+QVoHlg45HkGmhxM9AGo2+x2Ef52i5bwyjCBkSruiVfXAjKD+lMC/jz+1xLh2P9+ZIGbPtnFScrRdjrq'\n",
    "                  'FJEehq9CQ', \"origin\": \"https://logistics.amazon.in\",\n",
    "        \"referer\": f\"https://logistics.amazon.in/operations/execution/itineraries?selectedDay={yyyy_mm_dd}\"\n",
    "                   f\"&serviceAreaId={service_area_id}\",\n",
    "        \"sec-ch-ua\": '\"Chromium\";v=\"110\", \"Not A(Brand\";v=\"24\", \"Google Chrome\";v=\"110\"',\n",
    "        \"sec-ch-ua-mobile\": \"?0\",\n",
    "        \"sec-ch-ua-platform\": \"'Windows'\",\n",
    "        \"sec-fetch-dest\": \"empty\",\n",
    "        \"sec-fetch-mode\": \"cors\",\n",
    "        \"sec-fetch-site\": \"same-origin\",\n",
    "        \"user-agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "                      \"Chrome/110.0.0.0 Safari/537.36\"\n",
    "    }\n",
    "\n",
    "    response = requests.request(\"POST\", url, json=payload, headers=headers)\n",
    "    data = json.loads(response.text)\n",
    "    return pd.DataFrame(data['transportersLocation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c642a0-ea41-424a-b879-9afef733ba3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def manipulate_the_data(input_df):\n",
    "    df_raw = input_df.copy()\n",
    "    df_inter = df_raw.astype(object)\n",
    "    df_inter.fillna('No Data', inplace=True)\n",
    "    df = pd.concat([df_inter.drop(['stopProgress', 'packageStatus'], axis=1),\n",
    "                    df_inter['stopProgress'].apply(pd.Series), df_inter['packageStatus'].apply(pd.Series)], axis=1)\n",
    "    df['amazon_login_time'] = df['itineraryStartTime']. \\\n",
    "        apply(lambda x: datetime.fromtimestamp(int(str(x)[:10])) if x != 'No Data' else None)\n",
    "    df['location_updated_time'] = df['epochTimestamp']. \\\n",
    "        apply(lambda x: datetime.fromtimestamp(int(str(x)[:10])) if x != 'No Data' else None)\n",
    "    df['lastDriverEventTime'] = df['lastDriverEventTime']. \\\n",
    "        apply(lambda x: datetime.fromtimestamp(int(str(x)[:10])) if x != 'No Data' else None)\n",
    "    df['inactive_since'] = np.where(df['location_updated_time'] >= df['lastDriverEventTime'],\n",
    "                                    df['location_updated_time'],\n",
    "                                    df['lastDriverEventTime'])\n",
    "    df['inactive_since'].fillna(df['location_updated_time'], inplace=True)\n",
    "    df['inactive_hours'] = df['inactive_since']. \\\n",
    "        apply(lambda x: round((datetime.now() - x).total_seconds() / 3600, 2) if x != 'No Data' else x)\n",
    "    # df['inactive_hours'] = np.where((df['executionStatus'] == 'COMPLETE') | (df['driverSessionEnded'] is True),\n",
    "    #                                 0, df['inactive_hours'])\n",
    "    df['planned_end_time'] = df['timeRemainingSecs']. \\\n",
    "        apply(lambda x: datetime.now() + timedelta(seconds=x + 3) if x != 'No Data' else x)\n",
    "    df['stops_assigned'] = df['total']\n",
    "    df['stops_completed'] = np.where(df['actionedTimeWindowed'] == 0, df['completed'],\n",
    "                                     df['total'] - df['notStarted'] - df['inProgress'])\n",
    "    df['stops_at_risk'] = df['stopsAndPackagesByTaskAssessment']. \\\n",
    "        apply(lambda x: x['AT_RISK']['stopsImpacted'] if x else 'No Data')\n",
    "    df['pkgs_at_risk'] = df['stopsAndPackagesByTaskAssessment']. \\\n",
    "        apply(lambda x: x['AT_RISK']['packagesImpacted'] if x else 'No Data')\n",
    "    df['stops_ahead'] = df['stopsAndPackagesByTaskAssessment']. \\\n",
    "        apply(lambda x: x['AHEAD']['stopsImpacted'] if x else 'No Data')\n",
    "    df['pkgs_ahead'] = df['stopsAndPackagesByTaskAssessment']. \\\n",
    "        apply(lambda x: x['AHEAD']['packagesImpacted'] if x else 'No Data')\n",
    "\n",
    "    df['shift_hours_remaining'] = df['timeRemainingSecs']. \\\n",
    "        apply(lambda x: round(x / 3600, 2) if x != 'No Data' else x)\n",
    "\n",
    "    req_cols = ['updated_timestamp', 'station_code', 'transporterId', 'driver_name', 'amazon_login_time',\n",
    "                'planned_end_time', 'shift_hours_remaining', 'inactive_since', 'location_updated_time',\n",
    "                'lastDriverEventTime', 'inactive_hours', 'executionStatus', 'driverSessionEnded',\n",
    "                'stops_assigned', 'stops_completed', 'progressStatus', 'stops_at_risk', 'pkgs_at_risk',\n",
    "                'stops_ahead', 'pkgs_ahead', 'totalPackages', 'UNASSIGNED', 'DELIVERED', 'REMAINING',\n",
    "                'REATTEMPTABLE', 'UNDELIVERABLE', 'RETURNED']\n",
    "    req_cols.remove('location_updated_time')\n",
    "    req_cols.remove('lastDriverEventTime')\n",
    "\n",
    "    for col in req_cols:\n",
    "        if col not in df.columns:\n",
    "            df[col] = 0\n",
    "\n",
    "    final_df = df[req_cols]\n",
    "\n",
    "    final_df = final_df.rename(columns={'driverSessionEnded': 'driver_session_ended',\n",
    "                                        'transporterId': 'transporter_id',\n",
    "                                        'executionStatus': 'trip_status',\n",
    "                                        'lastDriverEventTime': 'last_driver_event_time',\n",
    "                                        'progressStatus': 'progress_status',\n",
    "                                        'totalPackages': 'pkgs_assigned',\n",
    "                                        'UNASSIGNED': 'pkgs_unassigned',\n",
    "                                        'DELIVERED': 'pkgs_delivered',\n",
    "                                        'REMAINING': 'pkgs_remaining',\n",
    "                                        'REATTEMPTABLE': 'pkgs_reattemptable',\n",
    "                                        'UNDELIVERABLE': 'pkgs_undeliverable',\n",
    "                                        'RETURNED': 'pkgs_rts_done'})\n",
    "\n",
    "    return final_df\n",
    "\n",
    "#5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934ad741-14f5-4e31-99fd-f18965f5263a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_the_df(input_df):\n",
    "    df = input_df.copy()\n",
    "    date_cols = ['amazon_login_time', 'inactive_since', 'planned_end_time']\n",
    "\n",
    "    for col in date_cols:\n",
    "        df[col] = df[col].apply(lambda x: x.strftime(\"%d/%m/%Y %H:%M\") if x != 'No Data' else x)\n",
    "\n",
    "    df['progress_status'] = df['progress_status'].str.replace('_', ' ').str.title()\n",
    "\n",
    "    inactive_df_cols = ['station_code', 'updated_timestamp', 'transporter_id', 'driver_name',\n",
    "                        'progress_status', 'amazon_login_time', 'inactive_since', 'shift_hours_remaining',\n",
    "                        'stops_assigned', 'stops_completed', 'stops_at_risk', 'pkgs_at_risk',\n",
    "                        'om_name', 'rm_name', 'client']\n",
    "    inactive_drivers = df[(df.inactive_hours >= 1) & (df.trip_status == 'DEPARTED')][inactive_df_cols].\\\n",
    "        reset_index(drop=True)\n",
    "\n",
    "    not_departed_df_cols = ['station_code', 'updated_timestamp', 'transporter_id', 'driver_name', 'amazon_login_time',\n",
    "                            'stops_assigned', 'stops_completed', 'om_name', 'rm_name', 'client']\n",
    "    not_departed_drivers = df[df.trip_status == 'NOT_DEPARTED'][not_departed_df_cols].reset_index(drop=True)\n",
    "    behinders_cols = ['station_code', 'updated_timestamp', 'transporter_id', 'driver_name', 'amazon_login_time',\n",
    "                      'planned_end_time', 'shift_hours_remaining', 'inactive_hours', 'stops_assigned',\n",
    "                      'stops_completed', 'stops_at_risk', 'om_name', 'rm_name', 'client']\n",
    "    behinders = df[df.stops_at_risk != 'No Data']\n",
    "    behinders = behinders[behinders.stops_at_risk >= 20][behinders_cols].reset_index(drop=True)\n",
    "\n",
    "    deliveries_cols = ['station_code', 'updated_timestamp', 'transporter_id', 'driver_name', 'amazon_login_time',\n",
    "                       'pkgs_delivered', 'pkgs_rts_done', 'pkgs_reattemptable', 'stops_assigned',\n",
    "                       'stops_completed', 'om_name', 'rm_name', 'client']\n",
    "\n",
    "    deliveries_df = df[deliveries_cols]\n",
    "    deliveries_df = deliveries_df.replace('', np.nan).fillna(0)\n",
    "\n",
    "    return {'inactive_drivers_df': inactive_drivers,\n",
    "            'not_departed_drivers_df': not_departed_drivers,\n",
    "            'behinders_df': behinders,\n",
    "            'deliveries_df': deliveries_df}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaee8d8e-8532-4b92-9b29-14dd084d5040",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_current_drivers_data(sample):\n",
    "    current_datetime = datetime.now(config.tz)\n",
    "    date_str = current_datetime.strftime(\"%Y-%m-%d\")\n",
    "    amazon_sites_df_raw = amazon_sites.create_amazon_sites_df(db=db_connector.connect_to_db(db_name='whizzard'))\n",
    "    amz_sites_fpath = f'/home/ubuntu/atom/{PROJECT_NAME}/amazon_sites.xlsx' \\\n",
    "        if config.ON_SERVER else f'../{PROJECT_NAME}/amazon_sites.xlsx'\n",
    "    amazon_sites_df = toolkit.save_or_retrieve_df_excel(input_df=amazon_sites_df_raw, fpath=amz_sites_fpath)\n",
    "    service_area_id_df = get_service_area_id()\n",
    "    sites_service_id_df = pd.merge(toolkit.snake_case_the_cols(amazon_sites_df),\n",
    "                                   service_area_id_df[['site_code', 'service_area_id']],\n",
    "                                   on='site_code', how='left')\n",
    "\n",
    "    stn_code_id_list = sites_service_id_df[['site_code', 'service_area_id']].to_numpy().tolist()\n",
    "    print(f'Total Number of Sites found : {len(stn_code_id_list)}')\n",
    "    stn_code_id_list = [row for row in stn_code_id_list if row[0] in ['HYDC', 'HYBH']] if sample else stn_code_id_list\n",
    "    print(f'Sample Size (Number of Sites) : {len(stn_code_id_list)}') if sample else print(end='')\n",
    "    drivers_df_list = []\n",
    "    num = 0\n",
    "    for stn, s_area_id in stn_code_id_list:\n",
    "        num += 1\n",
    "        try:\n",
    "            drivers_df = get_drivers_data(yyyy_mm_dd=date_str, service_area_id=s_area_id)\n",
    "            if not drivers_df.empty:\n",
    "                drivers_df.insert(loc=0, column='date', value=date_str)\n",
    "                drivers_df.insert(loc=1, column='updated_timestamp', value=current_datetime.strftime(\"%d/%m/%Y %H:00\"))\n",
    "                drivers_df.insert(loc=2, column='station_code', value=stn)\n",
    "                drivers_df_list.append(drivers_df)\n",
    "                print(f'{num}. {stn} - success', end=' ')\n",
    "            else:\n",
    "                print(f'{num}. {stn} - no_data', end=' ')\n",
    "        except Exception as err:\n",
    "            error_name = type(err).__name__\n",
    "            print(f'{num}. {stn} - {error_name}', end=' ')\n",
    "        if num % 8 == 0:\n",
    "            print()\n",
    "    print()\n",
    "\n",
    "    if drivers_df_list:\n",
    "        defaulters_df_raw = pd.concat(drivers_df_list)\n",
    "    else:\n",
    "        raise custom_errors.NoDataError\n",
    "\n",
    "    defaulters_df = manipulate_the_data(input_df=defaulters_df_raw)\n",
    "\n",
    "    site_details_df_raw = amazon_sites_df.drop(columns='Client Site Code')\n",
    "    site_details_df = site_details_df_raw.rename(columns={'Site Code': 'station_code'})\n",
    "    site_details_df = toolkit.snake_case_the_cols(input_df=site_details_df)\n",
    "    defaulters_df_final = pd.merge(defaulters_df, site_details_df, on='station_code', how='left')\n",
    "    splitted_dfs = split_the_df(input_df=defaulters_df_final)\n",
    "    # splitted_dfs['inactive_drivers_df'].columns = ['Site', 'Updated Time', 'Driver ID', 'Name', 'Status',\n",
    "    #                                                'Amz Login Time', 'Inactive From', 'Shift Hours Left',\n",
    "    #                                                'Total Stops', 'Completed Stops', 'Stops at Risk',\n",
    "    #                                                'Packages at Risk', 'OM', 'RM', 'Client']\n",
    "    # splitted_dfs['not_departed_drivers_df'].columns = ['Site', 'Updated Time', 'Driver ID', 'Name', 'Amz Login Time',\n",
    "    #                                                    'Total Stops', 'Completed Stops', 'OM', 'RM', 'Client']\n",
    "    # splitted_dfs['behinders_df'].columns = ['Site', 'Updated Time', 'Driver ID', 'Name', 'Amz Login Time',\n",
    "    #                                         'Shift Ends at', 'Shift Hours Left', 'Inactive Hours', 'Total Stops',\n",
    "    #                                         'Completed Stops', 'Stops at Risk', 'OM', 'RM', 'Client']\n",
    "    # splitted_dfs['deliveries_df'].columns = ['Site', 'Updated Time', 'Driver ID', 'Name', 'Amz Login Time',\n",
    "    #                                          'Delivered', 'RTS Done', 'Reattemptable', 'Total Stops',\n",
    "    #                                          'Completed Stops', 'OM', 'RM', 'Client']\n",
    "    return {'defaulters_df_final': defaulters_df_final,\n",
    "            'defaulters_df_raw': defaulters_df_raw,\n",
    "            'inactive_drivers_df': splitted_dfs['inactive_drivers_df'],\n",
    "            'not_departed_drivers_df': splitted_dfs['not_departed_drivers_df'],\n",
    "            'behinders_df': splitted_dfs['behinders_df'],\n",
    "            'deliveries_df': splitted_dfs['deliveries_df']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc253d39-d68e-4f7e-99f6-e1a94a6f3bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    load_dotenv()\n",
    "    data_folderpath = toolkit.create_folder(projectname=PROJECT_NAME, foldername='data')\n",
    "    final_output_fname = f'{datetime.now(config.tz).strftime(\"%Y-%m-%d_AmazonDefaulters_%HHrs\")}.xlsx'\n",
    "    final_output_fpath = data_folderpath + '/' + final_output_fname\n",
    "    t1 = time.time()  # execution start time for the python script\n",
    "\n",
    "    google_creds_fpath = f'/home/ubuntu/atom/{PROJECT_NAME}/google_account_credentials.json' if config.ON_SERVER \\\n",
    "        else f'/Users/Admin/PycharmProjects/{PROJECT_NAME}/google_account_credentials.json'\n",
    "\n",
    "    try:\n",
    "        print('\\n--------------------***--------------------\\n')\n",
    "        print(f'Execution Started at: {datetime.now(config.tz).strftime(\"%Y-%m-%d %H:%M:%S\")}')\n",
    "        final_dfs = get_current_drivers_data(sample=SAMPLE)\n",
    "        with pd.ExcelWriter(final_output_fpath, engine=None) as writer:\n",
    "            # final_dfs['defaulters_df_raw'].to_excel(writer, sheet_name='Raw', index=False)\n",
    "            # final_dfs['defaulters_df_final'].to_excel(writer, sheet_name='Data', index=False)\n",
    "            final_dfs['inactive_drivers_df'].to_excel(writer, sheet_name='Inactive', index=False)\n",
    "            final_dfs['not_departed_drivers_df'].to_excel(writer, sheet_name='Not Departed', index=False)\n",
    "            final_dfs['behinders_df'].to_excel(writer, sheet_name='Behinders', index=False)\n",
    "            final_dfs['deliveries_df'].to_excel(writer, sheet_name='Packages', index=False)\n",
    "            # df_raw.to_excel(writer, sheet_name='Raw-Data', index=False)\n",
    "\n",
    "        s3_foldername = 'whiz-amz-cortex-live'\n",
    "        s3_storage = s3c.connect_to_s3_storage(os.getenv('AWS_ACCESS_KEY_ID'),\n",
    "                                               os.getenv('AWS_SECRET_ACCESS_KEY'))\n",
    "        atom_bucket = s3_storage.Bucket('atom-s3')  # selecting a bucket from the s3 storage\n",
    "\n",
    "        if not SAMPLE and UPLOAD_TO_S3:\n",
    "            s3c.upload_to_s3(atom_bucket, s3_foldername, final_output_fpath, final_output_fname)\n",
    "\n",
    "        req_excel_files = sorted(s3c.get_all_excels(connected_bucket=atom_bucket, folder_name=s3_foldername),\n",
    "                                 reverse=True)[:40]\n",
    "        history_pkgs_df = s3c.concat_excel_sheets_to_df(connected_bucket=atom_bucket,\n",
    "                                                        excel_files=req_excel_files,\n",
    "                                                        sheet_num='Packages')\n",
    "\n",
    "        if UPDATE_GSHEET:\n",
    "            gc.upload_df_to_gsheets(creds_fpath=google_creds_fpath,\n",
    "                                    dataframe=final_dfs['inactive_drivers_df'],\n",
    "                                    file_name=\"amazon-cortex-live-data\",\n",
    "                                    sheet_name=\"Inactive\", formatting=False)\n",
    "            gc.upload_df_to_gsheets(creds_fpath=google_creds_fpath,\n",
    "                                    dataframe=final_dfs['not_departed_drivers_df'],\n",
    "                                    file_name=\"amazon-cortex-live-data\",\n",
    "                                    sheet_name=\"Not Departed\", formatting=False)\n",
    "            gc.upload_df_to_gsheets(creds_fpath=google_creds_fpath,\n",
    "                                    dataframe=final_dfs['behinders_df'],\n",
    "                                    file_name=\"amazon-cortex-live-data\",\n",
    "                                    sheet_name=\"Behinders\", formatting=False)\n",
    "            gc.upload_df_to_gsheets(creds_fpath=google_creds_fpath,\n",
    "                                    dataframe=final_dfs['deliveries_df'],\n",
    "                                    file_name=\"amazon-cortex-live-data\",\n",
    "                                    sheet_name=\"Packages\", formatting=False)\n",
    "            gc.upload_df_to_gsheets(creds_fpath=google_creds_fpath,\n",
    "                                    dataframe=history_pkgs_df,\n",
    "                                    file_name=\"amazon-cortex-live-data\",\n",
    "                                    sheet_name=\"History - Packages\", formatting=False)\n",
    "\n",
    "        print(f'Total Time for the Complete Execution : {(time.time() - t1) / 60:.3f} minutes')\n",
    "        print(f'Execution Completed at: {datetime.now(config.tz).strftime(\"%Y-%m-%d %H:%M:%S\")}')\n",
    "\n",
    "    except Exception as e:\n",
    "        message = f'Error occured while generating the report!\\nError:{type(e).__name__, e}'\n",
    "        print(message)\n",
    "        subj = f'Failed: {final_output_fname.replace(\".csv\", \"\")}'\n",
    "        toolkit.send_failure_email(send=SEND_FAIL_EMAIL, from_email=os.getenv('EMAIL_ID'),\n",
    "                                   pwd=os.getenv('EMAIL_PASSWORD'),\n",
    "                                   receiver_email='lafir.malim@whizzard.in',\n",
    "                                   email_subject=subj, email_message=message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70fe3d2-ab84-4a3e-b6bb-3481fd8e2ce1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883b8462-c878-46a7-9016-ba83fdecaee7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
