{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a65610-45b3-433d-beda-b26a33077743",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "\"\"\"\n",
    "This part of library are Python inbuild.\n",
    "- os Library for setup path for server\n",
    "- json Libray for converting API data to Python object\n",
    "- datetime and 'timedelta' for converting date object to Python object and 'datetime' is for geting current datetime\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "\"\"\"\n",
    "This part of library are 3rd party packages\n",
    "- Numpy and Pandas for data manipulation\n",
    "- requests is for fetching html or API data\n",
    "  POST and GET are two common requests method for fetching data. POST request is for sending request with data to server,\n",
    "  GET is common request we send to websites\n",
    "- dotenv is for reading environment variable inside Python script like user name, password or other private data\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "from whizzbox import (\n",
    "    amazon_sites,\n",
    "    config,\n",
    "    custom_errors,\n",
    "    db_connector,\n",
    "    gsheet_connector as gc,\n",
    "    s3_connector as s3c,\n",
    "    toolkit,\n",
    ")\n",
    "\n",
    "\"\"\"\n",
    "Wizzbox is private Python pacakge, that build by company it self for personal use\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4fb4901-74f7-4466-a324-5b1c49beb894",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_NAME = 'whiz-amz-defaulters'\n",
    "SAMPLE = True  # if it's a sample, no. of sites are limited\n",
    "SEND_FAIL_EMAIL = True\n",
    "UPDATE_GSHEET = False\n",
    "UPLOAD_TO_S3 = True\n",
    "\n",
    "\"\"\"\n",
    "all capital latter is writing code best practive in Python,\n",
    "that indecate should not remove or change this variables.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e26490-206e-49bd-a07d-6307545bd26d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d1bfab90-7453-4cd4-934e-b16ceecfbc57",
   "metadata": {},
   "source": [
    "#### **Function 1 code** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a1c582-5603-4f43-a6c8-64f43542a12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# noinspection SpellCheckingInspection  (Function 1)\n",
    "def get_service_area_id():\n",
    "    \"\"\"\n",
    "    Retrieves service area IDs from an API endpoint.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing service area information.\n",
    "    \"\"\"\n",
    "\n",
    "    payload = \"\"\n",
    "    headers = {\"cookie\": \"session-id=260-6428466-7223316; session-id-time=2082787201l; \"\n",
    "                         \"session-token=GbElPtRlx37WtbcOToTC3cRl2schxCQbGfw9PermyrbyTyonOdmMyDvN9xAn3Y1u\"\n",
    "                         \"urDFHnNpykoR0H9l8R%2Bw0pqKOD14Zwtk4QClUVQpQsIItI6ETeBDN0M5SpFCUku0BGvpR2xSOFJN\"\n",
    "                         \"nMUMDfYiflLntxjcLhKYtat1HekaJ2LZEwTlJ3dL6%2Fs%2B9U37Zu8JIQq%2FK7qPG5HFr4SwvqwIoQ\"\n",
    "                         \"pyvIMoFI7ZbtIDa1gNSij8nXYZmqmACRQ0FUVbIFHYlcHjlwmkXzg%3D\"}\n",
    "\n",
    "    url = \"https://logistics.amazon.in/flex/api/getOperationalRegions\"\n",
    "    response = requests.get(url, data=payload, headers=headers)\n",
    "\n",
    "    data = json.loads(response.text)\n",
    "    req_data = data\n",
    "    api_df_raw = pd.DataFrame(req_data)\n",
    "    \n",
    "    \"\"\"\n",
    "    Payload, Header, and URl is sending request materials.\n",
    "    Header conferm, we get write data based on our requirements and URL is parrent path.\n",
    "    Request library under the hood make this into url raw string, that server can understand.\n",
    "    \n",
    "    We converted APIs json response to Python json object help of json.loads()\n",
    "    We converted json object to pandas data frame 'api_df_raw'\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "\n",
    "    # Explode the 'basicServiceAreas' column into multiple rows\n",
    "    api_df = api_df_raw.explode('basicServiceAreas')\n",
    "\n",
    "    # Split the exploded column into separate columns\n",
    "    api_df_inter = pd.concat(\n",
    "        [\n",
    "            api_df.drop(['basicServiceAreas'], axis=1),\n",
    "            api_df['basicServiceAreas'].apply(pd.Series)\n",
    "        ],\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    # Concatenate the necessary columns for the final DataFrame\n",
    "    df_final = pd.concat(\n",
    "        [\n",
    "            api_df_inter.drop(['pickUpLocationAddress', 'pickUpLocation'], axis=1),\n",
    "            api_df_inter['pickUpLocationAddress'].apply(pd.Series),\n",
    "            api_df_inter['pickUpLocation'].apply(pd.Series)\n",
    "        ],\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    # Select the desired columns and rename them\n",
    "    final_df = df_final[['defaultStationCode', 'serviceAreaID', 'regionID', 'regionName',\n",
    "                         'state', 'postalCode', 'longitude', 'latitude', 'active']]\n",
    "    final_df.columns = ['site_code', 'service_area_id', 'region_id', 'region_name',\n",
    "                        'state', 'postal_code', 'longitude', 'latitude', 'active']\n",
    "    \n",
    "    \"\"\"\n",
    "    This part doing data manupulation task based on business requirements, and return a final clean data frame\n",
    "    \"\"\"\n",
    "\n",
    "    return final_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3451704-6a02-4c67-9b54-c2fe896788cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function 1 code break down\n",
    "import json\n",
    "import requests\n",
    "\n",
    "\n",
    "url = \"https://logistics.amazon.in/flex/api/getOperationalRegions\"\n",
    "payload = \"\"\n",
    "headers = {\"cookie\": \"session-id=260-6428466-7223316; session-id-time=2082787201l; \"\n",
    "                     \"session-token=GbElPtRlx37WtbcOToTC3cRl2schxCQbGfw9PermyrbyTyonOdmMyDvN9xAn3Y1u\"\n",
    "                     \"urDFHnNpykoR0H9l8R%2Bw0pqKOD14Zwtk4QClUVQpQsIItI6ETeBDN0M5SpFCUku0BGvpR2xSOFJN\"\n",
    "                     \"nMUMDfYiflLntxjcLhKYtat1HekaJ2LZEwTlJ3dL6%2Fs%2B9U37Zu8JIQq%2FK7qPG5HFr4SwvqwIoQ\"\n",
    "                     \"pyvIMoFI7ZbtIDa1gNSij8nXYZmqmACRQ0FUVbIFHYlcHjlwmkXzg%3D\"}\n",
    "\n",
    "response = requests.get(url, data=payload, headers=headers)\n",
    "\n",
    "data = json.loads(response.text)\n",
    "req_data = data\n",
    "\n",
    "\"\"\"\n",
    "# Display json data\n",
    "file_path = \"f1_get_service_area_id.json\"\n",
    "with open(file_path, 'w') as file:\n",
    "    json.dump(data, file)\n",
    "    print(f\"===== Save json data on f1_get_service_area_id.json file, Jupyerlab have data limitaion ======\")\n",
    "\"\"\" \n",
    "\n",
    "api_df_raw = pd.DataFrame(req_data)\n",
    "api_df = api_df_raw.explode('basicServiceAreas')\n",
    "print(f\"===== api_df data frame raws and columns: {api_df.shape}\")\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "This code is restructure the data base and return a clean data frame\n",
    "\"\"\"\n",
    "# Split the exploded column into separate columns\n",
    "api_df_inter = pd.concat(\n",
    "    [\n",
    "        api_df.drop(['basicServiceAreas'], axis=1),\n",
    "        api_df['basicServiceAreas'].apply(pd.Series)\n",
    "    ],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Concatenate the necessary columns for the final DataFrame\n",
    "df_final = pd.concat(\n",
    "    [\n",
    "        api_df_inter.drop(['pickUpLocationAddress', 'pickUpLocation'], axis=1),\n",
    "        api_df_inter['pickUpLocationAddress'].apply(pd.Series),\n",
    "        api_df_inter['pickUpLocation'].apply(pd.Series)\n",
    "    ],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Select the desired columns and rename them\n",
    "final_df = df_final[['defaultStationCode', 'serviceAreaID', 'regionID', 'regionName',\n",
    "                     'state', 'postalCode', 'longitude', 'latitude', 'active']]\n",
    "final_df.columns = ['site_code', 'service_area_id', 'region_id', 'region_name',\n",
    "                    'state', 'postal_code', 'longitude', 'latitude', 'active']\n",
    "\n",
    "print(f\"===== final_df data frame raws and columns{final_df.shape} ====\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2919166d-78d3-4983-80d7-586b5ea6a5ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eabf3ec-c85f-4a09-a88e-994cf42a4a60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1dbd20b4-7997-47fb-b62e-9d85891ef83e",
   "metadata": {},
   "source": [
    "#### **Function 2 code** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa325fdc-371b-4d06-bb17-644cf557f534",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_drivers_data(yyyy_mm_dd, service_area_id):\n",
    "    \"\"\"\n",
    "    Retrieves driver data for a specific date and service area.\n",
    "\n",
    "    Args:\n",
    "        yyyy_mm_dd (str): The date in the format \"YYYY-MM-DD\".\n",
    "        service_area_id (str): The service area ID.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing driver data.\n",
    "    \"\"\"\n",
    "\n",
    "    querystring = {\"localDate\": f\"{yyyy_mm_dd}\", \"serviceAreaId\": f\"{service_area_id}\"}\n",
    "\n",
    "    payload = \"\"\n",
    "    headers = {\n",
    "        \"cookie\": 'session-id=258-5953668-1245245; ubid-acbin=261-0863908-3910552; '\n",
    "                  'lc-acbin=en_IN; x-acbin=\"57E9eKz@7A@To6Vddbaqer5jgdCIqlKfP2T8kM6wKCICy?gDv7vle?RKDpKPgjZK\";'\n",
    "                  ' at-acbin=Atza|IwEBIJk3W45PoKDQRfBKht6R7k4X7GddLPM54_esWjs8dHrH6815nPGyW5DMuqexf1w0bDDz10cOG'\n",
    "                  'JJEgdOwPR3Us-wmX2ChldzceR7pz7ciQRrZdKYRDV5A-L0kRQRtlbXvJvefFPPYLQUohVZ59kgsrTlSnAMO2MUVSxI8q'\n",
    "                  'sBOXW2fPqGjf3o3IIx_f3T8ASPkGXUQcS2C9yTpnncFHoyCgAcCfs0dyQBBJzFYedq3XUQLVg; sess-at-acbin'\n",
    "                  '=\"ckW6tTbMs1yigrh3KsKaTVeibCPC2YHIFdC5PnL4F+Q=\"; x-amz-log-portal-locale=en-IN; session'\n",
    "                  '-id-time=2082787201l; session-token=F8JjOaEew4Id8WCibAXIjaVT8YkIKwPtKpnJQf0jBIrqTbaLb4Y2Kv47'\n",
    "                  'nJXwvdCTKygovIhu+XF4jOcoVSZ7ImRYrmDOYEUTdlQ6uYK2r9jmnmHdDBLkl/HGcgDFFKrrJidRt61RI4MJ1PGgzhW'\n",
    "                  'OIWRMV7OAW+QVoHlg45HkGmhxM9AGo2+x2Ef52i5bwyjCBkSruiVfXAjKD+lMC/jz+1xLh2P9+ZIGbPtnFScrRdjrq'\n",
    "                  'FJEehq9CQ',\n",
    "        \"authority\": \"logistics.amazon.in\",\n",
    "        \"accept\": \"application/json, text/plain, */*\",\n",
    "        \"accept-language\": \"en-GB,en-US;q=0.9,en;q=0.8\",\n",
    "        \"referer\": f\"https://logistics.amazon.in/operations/execution/itineraries?selectedDay={yyyy_mm_dd}\"\n",
    "                   f\"&serviceAreaId={service_area_id}\",\n",
    "        \"sec-ch-ua\": '\"Not_A Brand\";v=\"99\", \"Google Chrome\";v=\"109\", \"Chromium\";v=\"109\"',\n",
    "        \"sec-ch-ua-mobile\": \"?0\",\n",
    "        \"sec-ch-ua-platform\": '\"Windows\"',\n",
    "        \"sec-fetch-dest\": \"empty\",\n",
    "        \"sec-fetch-mode\": \"cors\",\n",
    "        \"sec-fetch-site\": \"same-origin\",\n",
    "        \"user-agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "                      \"Chrome/109.0.0.0 Safari/537.36\"\n",
    "    }\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    url = \"https://logistics.amazon.in/operations/execution/api/summaries\"\n",
    "    response = requests.get(url, data=payload, headers=headers, params=querystring)\n",
    "    \n",
    "    data = json.loads(response.text)\n",
    "    driver_df = pd.DataFrame(data['transporters'])\n",
    "    \n",
    "    \"\"\"\n",
    "    This part same as function 1,\n",
    "    that sending request with querystring, payload, and headers.\n",
    "    After sending request it convert json response to pandas data frame\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    if not driver_df.empty:\n",
    "        # Retrieve unique driver IDs\n",
    "        driver_ids = list(driver_df['transporterId'].unique())\n",
    "\n",
    "        itinerary_df = pd.DataFrame(data['itinerarySummaries'])\n",
    "        packages_df = pd.DataFrame(data['transporterPackageSummaries']) if data['transporterPackageSummaries'] \\\n",
    "            else pd.DataFrame()\n",
    "\n",
    "        # Get drivers' location information\n",
    "        location_df = get_drivers_location(yyyy_mm_dd, service_area_id, driver_ids)\n",
    "\n",
    "        # Merge driver, itinerary, and location data\n",
    "        merged_df = pd.merge(itinerary_df, pd.merge(driver_df, location_df, on='transporterId', how='left'),\n",
    "                             on='transporterId', how='left')\n",
    "        merged_df['driver_name'] = merged_df['firstName'].str.title() + ' ' + merged_df['lastName'].str.title()\n",
    "\n",
    "        if not packages_df.empty:\n",
    "            # Merge package data if available\n",
    "            merged_df_final = pd.merge(merged_df, packages_df[['transporterId', 'packageStatus']],\n",
    "                                       on='transporterId', how='left')\n",
    "        else:\n",
    "            merged_df_final = merged_df.copy()\n",
    "\n",
    "        df_final = merged_df_final.copy()\n",
    "    else:\n",
    "        df_final = pd.DataFrame()\n",
    "        \n",
    "    \n",
    "    \"\"\"\n",
    "    This part check if data frame is contain data or not, if dat is there then it doing data manipulation task.\n",
    "    After that it return pandas data frame for future process.\n",
    "    \"\"\"\n",
    "\n",
    "    return df_final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac32f13-06f6-44b8-a4fd-b3709270902c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906ac47e-2f2c-4a91-8d63-3802f749b1a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b00baa6b-e78a-4a47-8880-9978676951f8",
   "metadata": {},
   "source": [
    "#### **Function 3 code** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70fe3d2-ab84-4a3e-b6bb-3481fd8e2ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_drivers_location(yyyy_mm_dd, service_area_id, transporter_ids: list):\n",
    "    \"\"\"\n",
    "    Retrieves the location data of drivers for a specific date, service area, and transporter IDs.\n",
    "\n",
    "    Args:\n",
    "        yyyy_mm_dd (str): The date in the format \"YYYY-MM-DD\".\n",
    "        service_area_id (str): The service area ID.\n",
    "        transporter_ids (list): List of transporter IDs.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing the drivers' location data.\n",
    "    \"\"\"\n",
    "\n",
    "    # Prepare the payload with transporter IDs\n",
    "    payload = {\n",
    "        \"transporterIds\": transporter_ids\n",
    "    }\n",
    "\n",
    "    # Prepare the headers for the request\n",
    "    headers = {\n",
    "        \"authority\": \"logistics.amazon.in\",\n",
    "        \"accept\": \"application/json, text/plain, */*\",\n",
    "        \"accept-language\": \"en-GB,en-US;q=0.9,en;q=0.8\",\n",
    "        \"content-type\": \"application/json;charset=UTF-8\",\n",
    "        \"cookie\": 'session-id=258-5953668-1245245; ubid-acbin=261-0863908-3910552; '\n",
    "                  'lc-acbin=en_IN; x-acbin=\"57E9eKz@7A@To6Vddbaqer5jgdCIqlKfP2T8kM6wKCICy?gDv7vle?RKDpKPgjZK\";'\n",
    "                  ' at-acbin=Atza|IwEBIJk3W45PoKDQRfBKht6R7k4X7GddLPM54_esWjs8dHrH6815nPGyW5DMuqexf1w0bDDz10cOG'\n",
    "                  'JJEgdOwPR3Us-wmX2ChldzceR7pz7ciQRrZdKYRDV5A-L0kRQRtlbXvJvefFPPYLQUohVZ59kgsrTlSnAMO2MUVSxI8q'\n",
    "                  'sBOXW2fPqGjf3o3IIx_f3T8ASPkGXUQcS2C9yTpnncFHoyCgAcCfs0dyQBBJzFYedq3XUQLVg; sess-at-acbin'\n",
    "                  '=\"ckW6tTbMs1yigrh3KsKaTVeibCPC2YHIFdC5PnL4F+Q=\"; x-amz-log-portal-locale=en-IN; session'\n",
    "                  '-id-time=2082787201l; session-token=F8JjOaEew4Id8WCibAXIjaVT8YkIKwPtKpnJQf0jBIrqTbaLb4Y2Kv47'\n",
    "                  'nJXwvdCTKygovIhu+XF4jOcoVSZ7ImRYrmDOYEUTdlQ6uYK2r9jmnmHdDBLkl/HGcgDFFKrrJidRt61RI4MJ1PGgzhW'\n",
    "                  'OIWRMV7OAW+QVoHlg45HkGmhxM9AGo2+x2Ef52i5bwyjCBkSruiVfXAjKD+lMC/jz+1xLh2P9+ZIGbPtnFScrRdjrq'\n",
    "                  'FJEehq9CQ', \"origin\": \"https://logistics.amazon.in\",\n",
    "        \"referer\": f\"https://logistics.amazon.in/operations/execution/itineraries?selectedDay={yyyy_mm_dd}\"\n",
    "                   f\"&serviceAreaId={service_area_id}\",\n",
    "        \"sec-ch-ua\": '\"Chromium\";v=\"110\", \"Not A(Brand\";v=\"24\", \"Google Chrome\";v=\"110\"',\n",
    "        \"sec-ch-ua-mobile\": \"?0\",\n",
    "        \"sec-ch-ua-platform\": \"'Windows'\",\n",
    "        \"sec-fetch-dest\": \"empty\",\n",
    "        \"sec-fetch-mode\": \"cors\",\n",
    "        \"sec-fetch-site\": \"same-origin\",\n",
    "        \"user-agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "                      \"Chrome/110.0.0.0 Safari/537.36\"\n",
    "    }\n",
    "\n",
    "    # Send a POST request to retrieve the drivers' location data\n",
    "    url = \"https://logistics.amazon.in/operations/execution/api/transporters/locationUpdate\"\n",
    "    response = requests.request(\"POST\", url, json=payload, headers=headers)\n",
    "    data = json.loads(response.text)\n",
    "    \"\"\"\n",
    "    This part is same as function 1 and function 2 like sending request with querystring, payload, and headers.\n",
    "    And also make a pandas dataframe.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    # Convert the retrieved data into a DataFrame\n",
    "    df = pd.DataFrame(data['transportersLocation'])\n",
    "    \"\"\"\n",
    "    This part convert json object to pandas data frame only 'transportersLocation', that will found on API response\n",
    "    \"\"\"\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883b8462-c878-46a7-9016-ba83fdecaee7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536a2ed1-983a-4c40-ba0e-ab461a3f6d63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2dc30d10-b611-4636-a64d-9b01215efd2d",
   "metadata": {},
   "source": [
    "#### **Function 4 code** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e954dc60-e358-403d-bfdd-ec6e9aa364d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def manipulate_the_data(input_df):\n",
    "    \"\"\"\n",
    "    Manipulates the input DataFrame to derive additional columns and rename existing columns.\n",
    "\n",
    "    Args:\n",
    "        input_df (pandas.DataFrame): The input DataFrame to manipulate.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: The manipulated DataFrame.\n",
    "    \"\"\"\n",
    "    # Create a copy of the input DataFrame\n",
    "    df_raw = input_df.copy()\n",
    "\n",
    "    # Convert the DataFrame to object type\n",
    "    df_inter = df_raw.astype(object)\n",
    "\n",
    "    # Fill missing values with 'No Data'\n",
    "    df_inter.fillna('No Data', inplace=True)\n",
    "\n",
    "    # Concatenate columns and expand nested columns\n",
    "    df = pd.concat([df_inter.drop(['stopProgress', 'packageStatus'], axis=1),\n",
    "                    df_inter['stopProgress'].apply(pd.Series),\n",
    "                    df_inter['packageStatus'].apply(pd.Series)], axis=1)\n",
    "\n",
    "    # Convert timestamp columns to datetime format\n",
    "    df['amazon_login_time'] = df['itineraryStartTime'].apply(\n",
    "        lambda x: datetime.fromtimestamp(int(str(x)[:10])) if x != 'No Data' else None\n",
    "    )\n",
    "    df['location_updated_time'] = df['epochTimestamp'].apply(\n",
    "        lambda x: datetime.fromtimestamp(int(str(x)[:10])) if x != 'No Data' else None\n",
    "    )\n",
    "    df['lastDriverEventTime'] = df['lastDriverEventTime'].apply(\n",
    "        lambda x: datetime.fromtimestamp(int(str(x)[:10])) if x != 'No Data' else None\n",
    "    )\n",
    "\n",
    "    # Calculate inactive_since column\n",
    "    df['inactive_since'] = np.where(df['location_updated_time'] >= df['lastDriverEventTime'],\n",
    "                                    df['location_updated_time'],\n",
    "                                    df['lastDriverEventTime'])\n",
    "    df['inactive_since'].fillna(df['location_updated_time'], inplace=True)\n",
    "\n",
    "    # Calculate inactive_hours column\n",
    "    df['inactive_hours'] = df['inactive_since'].apply(\n",
    "        lambda x: round((datetime.now() - x).total_seconds() / 3600, 2) if x != 'No Data' else x\n",
    "    )\n",
    "\n",
    "    # Calculate planned_end_time column\n",
    "    df['planned_end_time'] = df['timeRemainingSecs'].apply(\n",
    "        lambda x: datetime.now() + timedelta(seconds=x + 3) if x != 'No Data' else x\n",
    "    )\n",
    "\n",
    "    # Calculate stops_completed column\n",
    "    df['stops_completed'] = np.where(df['actionedTimeWindowed'] == 0, df['completed'],\n",
    "                                     df['total'] - df['notStarted'] - df['inProgress'])\n",
    "\n",
    "    # Extract data from stopsAndPackagesByTaskAssessment column\n",
    "    df['stops_at_risk'] = df['stopsAndPackagesByTaskAssessment'].apply(\n",
    "        lambda x: x['AT_RISK']['stopsImpacted'] if x else 'No Data'\n",
    "    )\n",
    "    df['pkgs_at_risk'] = df['stopsAndPackagesByTaskAssessment'].apply(\n",
    "        lambda x: x['AT_RISK']['packagesImpacted'] if x else 'No Data'\n",
    "    )\n",
    "    df['stops_ahead'] = df['stopsAndPackagesByTaskAssessment'].apply(\n",
    "        lambda x: x['AHEAD']['stopsImpacted'] if x else 'No Data'\n",
    "    )\n",
    "    df['pkgs_ahead'] = df['stopsAndPackagesByTaskAssessment'].apply(\n",
    "        lambda x: x['AHEAD']['packagesImpacted'] if x else 'No Data'\n",
    "    )\n",
    "\n",
    "    # Calculate shift_hours_remaining column\n",
    "    df['shift_hours_remaining'] = df['timeRemainingSecs'].apply(\n",
    "        lambda x: round(x / 3600, 2) if x != 'No Data' else x\n",
    "    )\n",
    "    \n",
    "    \n",
    "    # Define required columns\n",
    "    req_cols = ['updated_timestamp', 'station_code', 'transporterId', 'driver_name', 'amazon_login_time',\n",
    "                'planned_end_time', 'shift_hours_remaining', 'inactive_since', 'location_updated_time',\n",
    "                'lastDriverEventTime', 'inactive_hours', 'executionStatus', 'driverSessionEnded',\n",
    "                'stops_assigned', 'stops_completed', 'progressStatus', 'stops_at_risk', 'pkgs_at_risk',\n",
    "                'stops_ahead', 'pkgs_ahead', 'totalPackages', 'UNASSIGNED', 'DELIVERED', 'REMAINING',\n",
    "                'REATTEMPTABLE', 'UNDELIVERABLE', 'RETURNED']\n",
    "\n",
    "    # Remove redundant columns\n",
    "    req_cols.remove('location_updated_time')\n",
    "    req_cols.remove('lastDriverEventTime')\n",
    "\n",
    "    # Add missing columns with default value 0\n",
    "    for col in req_cols:\n",
    "        if col not in df.columns:\n",
    "            df[col] = 0\n",
    "\n",
    "    # Select the final columns\n",
    "    final_df = df[req_cols]\n",
    "\n",
    "    # Rename the columns\n",
    "    final_df = final_df.rename(columns={'driverSessionEnded': 'driver_session_ended',\n",
    "                                        'transporterId': 'transporter_id',\n",
    "                                        'executionStatus': 'trip_status',\n",
    "                                        'lastDriverEventTime': 'last_driver_event_time',\n",
    "                                        'progressStatus': 'progress_status',\n",
    "                                        'totalPackages': 'pkgs_assigned',\n",
    "                                        'UNASSIGNED': 'pkgs_unassigned',\n",
    "                                        'DELIVERED': 'pkgs_delivered',\n",
    "                                        'REMAINING': 'pkgs_remaining',\n",
    "                                        'REATTEMPTABLE': 'pkgs_reattemptable',\n",
    "                                        'UNDELIVERABLE': 'pkgs_undeliverable',\n",
    "                                        'RETURNED': 'pkgs_rts_done'})\n",
    "    \n",
    "    return final_df\n",
    "\n",
    "    \"\"\"\n",
    "    This total function taking data frame as input, and cleaning it based on requirements.\n",
    "    After that it return a final dataframe for future process.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28bfcd49-11f7-4501-bb9c-4d8f394f1436",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67099339-d0a9-4d72-88e9-de1af789f3ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6db7510c-528e-437f-a204-2fb4d37c6a53",
   "metadata": {},
   "source": [
    "#### **Function 5 code** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed034be7-2868-488e-a7ff-69dd373e0a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_the_df(input_df):\n",
    "    \"\"\"\n",
    "    Split the input DataFrame into multiple DataFrames based on specific conditions and return a dictionary\n",
    "    containing the split DataFrames.\n",
    "\n",
    "    Args:\n",
    "        input_df (pd.DataFrame): Input DataFrame to be split.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the split DataFrames.\n",
    "\n",
    "    \"\"\"\n",
    "    df = input_df.copy()\n",
    "\n",
    "    # Format date columns\n",
    "    date_cols = ['amazon_login_time', 'inactive_since', 'planned_end_time']\n",
    "    for col in date_cols:\n",
    "        df[col] = df[col].apply(lambda x: x.strftime(\"%d/%m/%Y %H:%M\") if x != 'No Data' else x)\n",
    "\n",
    "    # Format progress status column\n",
    "    df['progress_status'] = df['progress_status'].str.replace('_', ' ').str.title()\n",
    "\n",
    "    # Split inactive drivers DataFrame\n",
    "    inactive_df_cols = ['station_code', 'updated_timestamp', 'transporter_id', 'driver_name',\n",
    "                        'progress_status', 'amazon_login_time', 'inactive_since', 'shift_hours_remaining',\n",
    "                        'stops_assigned', 'stops_completed', 'stops_at_risk', 'pkgs_at_risk',\n",
    "                        'om_name', 'rm_name', 'client']\n",
    "    inactive_drivers = df[(df.inactive_hours >= 1) & (df.trip_status == 'DEPARTED')][inactive_df_cols].reset_index(drop=True)\n",
    "\n",
    "    # Split not departed drivers DataFrame\n",
    "    not_departed_df_cols = ['station_code', 'updated_timestamp', 'transporter_id', 'driver_name', 'amazon_login_time',\n",
    "                            'stops_assigned', 'stops_completed', 'om_name', 'rm_name', 'client']\n",
    "    not_departed_drivers = df[df.trip_status == 'NOT_DEPARTED'][not_departed_df_cols].reset_index(drop=True)\n",
    "\n",
    "    # Split behinders DataFrame\n",
    "    behinders_cols = ['station_code', 'updated_timestamp', 'transporter_id', 'driver_name', 'amazon_login_time',\n",
    "                      'planned_end_time', 'shift_hours_remaining', 'inactive_hours', 'stops_assigned',\n",
    "                      'stops_completed', 'stops_at_risk', 'om_name', 'rm_name', 'client']\n",
    "    behinders = df[df.stops_at_risk != 'No Data']\n",
    "    behinders = behinders[behinders.stops_at_risk >= 20][behinders_cols].reset_index(drop=True)\n",
    "\n",
    "    # Split deliveries DataFrame\n",
    "    deliveries_cols = ['station_code', 'updated_timestamp', 'transporter_id', 'driver_name', 'amazon_login_time',\n",
    "                       'pkgs_delivered', 'pkgs_rts_done', 'pkgs_reattemptable', 'stops_assigned',\n",
    "                       'stops_completed', 'om_name', 'rm_name', 'client']\n",
    "    deliveries_df = df[deliveries_cols]\n",
    "    deliveries_df = deliveries_df.replace('', np.nan).fillna(0)\n",
    "\n",
    "    # Return the split DataFrames as a dictionary\n",
    "    return {'inactive_drivers_df': inactive_drivers,\n",
    "            'not_departed_drivers_df': not_departed_drivers,\n",
    "            'behinders_df': behinders,\n",
    "            'deliveries_df': deliveries_df}\n",
    "\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    This function is same as function 4 that take pandas data frame as input,\n",
    "    manipulation data and return a Python dictionary for future process.\n",
    "    \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3240cd21-2bbd-47be-95b9-c15c84783d9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2da66d-c328-47df-9df5-3403d97ba4cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d966e31c-15ab-42e9-9941-b6bccee8364c",
   "metadata": {},
   "source": [
    "#### **Function 6 code** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb45112-9a08-4333-8581-33f65fd2f68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_current_drivers_data(sample):\n",
    "    \"\"\"\n",
    "    Retrieve current drivers' data for specified sample size or all sites.\n",
    "\n",
    "    Args:\n",
    "        sample (bool): Flag indicating whether to use a sample size or retrieve data for all sites.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the retrieved drivers' data in different DataFrames.\n",
    "\n",
    "    Raises:\n",
    "        custom_errors.NoDataError: If no data is retrieved for any site.\n",
    "\n",
    "    \"\"\"\n",
    "    current_datetime = datetime.now(config.tz)\n",
    "    date_str = current_datetime.strftime(\"%Y-%m-%d\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    amazon_sites_df_raw = amazon_sites.create_amazon_sites_df(\n",
    "        db=db_connector.connect_to_db(db_name='whizzard'))\n",
    "    amz_sites_fpath = f'/home/ubuntu/atom/{PROJECT_NAME}/amazon_sites.xlsx' \\\n",
    "        if config.ON_SERVER else f'../{PROJECT_NAME}/amazon_sites.xlsx'\n",
    "    amazon_sites_df = toolkit.save_or_retrieve_df_excel(\n",
    "        input_df=amazon_sites_df_raw, fpath=amz_sites_fpath)\n",
    "    service_area_id_df = get_service_area_id()\n",
    "    sites_service_id_df = pd.merge(toolkit.snake_case_the_cols(amazon_sites_df),\n",
    "                                   service_area_id_df[[\n",
    "                                       'site_code', 'service_area_id']],\n",
    "                                   on='site_code', how='left')\n",
    "\n",
    "    stn_code_id_list = sites_service_id_df[[\n",
    "        'site_code', 'service_area_id']].to_numpy().tolist()\n",
    "    print(f'Total Number of Sites found : {len(stn_code_id_list)}')\n",
    "    stn_code_id_list = [row for row in stn_code_id_list if row[0] in [\n",
    "        'HYDC', 'HYBH']] if sample else stn_code_id_list\n",
    "    print(f'Sample Size (Number of Sites) : {len(stn_code_id_list)}') if sample else print(\n",
    "        end='')\n",
    "    \n",
    "    \n",
    "    \n",
    "    drivers_df_list = []\n",
    "    num = 0\n",
    "    for stn, s_area_id in stn_code_id_list:\n",
    "        num += 1\n",
    "        try:\n",
    "            drivers_df = get_drivers_data(\n",
    "                yyyy_mm_dd=date_str, service_area_id=s_area_id)\n",
    "            if not drivers_df.empty:\n",
    "                drivers_df.insert(loc=0, column='date', value=date_str)\n",
    "                drivers_df.insert(loc=1, column='updated_timestamp',\n",
    "                                  value=current_datetime.strftime(\"%d/%m/%Y %H:00\"))\n",
    "                drivers_df.insert(loc=2, column='station_code', value=stn)\n",
    "                drivers_df_list.append(drivers_df)\n",
    "                print(f'{num}. {stn} - success', end=' ')\n",
    "            else:\n",
    "                print(f'{num}. {stn} - no_data', end=' ')\n",
    "        except Exception as err:\n",
    "            error_name = type(err).__name__\n",
    "            print(f'{num}. {stn} - {error_name}', end=' ')\n",
    "        if num % 8 == 0:\n",
    "            print()\n",
    "    print()\n",
    "\n",
    "    if drivers_df_list:\n",
    "        defaulters_df_raw = pd.concat(drivers_df_list)\n",
    "    else:\n",
    "        raise custom_errors.NoDataError\n",
    "\n",
    "    defaulters_df = manipulate_the_data(input_df=defaulters_df_raw)\n",
    "\n",
    "    site_details_df_raw = amazon_sites_df.drop(columns='Client Site Code')\n",
    "    site_details_df = site_details_df_raw.rename(\n",
    "        columns={'Site Code': 'station_code'})\n",
    "    site_details_df = toolkit.snake_case_the_cols(input_df=site_details_df)\n",
    "    defaulters_df_final = pd.merge(\n",
    "        defaulters_df, site_details_df, on='station_code', how='left')\n",
    "    splitted_dfs = split_the_df(input_df=defaulters_df_final)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # splitted_dfs['inactive_drivers_df'].columns = ['Site', 'Updated Time', 'Driver ID', 'Name', 'Status',\n",
    "    #                                                'Amz Login Time', 'Inactive From', 'Shift Hours Left',\n",
    "    #                                                'Total Stops', 'Completed Stops', 'Stops at Risk',\n",
    "    #                                                'Packages at Risk', 'OM', 'RM', 'Client']\n",
    "    # splitted_dfs['not_departed_drivers_df'].columns = ['Site', 'Updated Time', 'Driver ID', 'Name', 'Amz Login Time',\n",
    "    #                                                    'Total Stops', 'Completed Stops', 'OM', 'RM', 'Client']\n",
    "    # splitted_dfs['behinders_df'].columns = ['Site', 'Updated Time', 'Driver ID', 'Name', 'Amz Login Time',\n",
    "    #                                         'Shift Ends at', 'Shift Hours Left', 'Inactive Hours', 'Total Stops',\n",
    "    #                                         'Completed Stops', 'Stops at Risk', 'OM', 'RM', 'Client']\n",
    "    # splitted_dfs['deliveries_df'].columns = ['Site', 'Updated Time', 'Driver ID', 'Name', 'Amz Login Time',\n",
    "    #                                          'Delivered', 'RTS Done', 'Reattemptable', 'Total Stops',\n",
    "    #                                          'Completed Stops', 'OM', 'RM', 'Client']\n",
    "    \n",
    "    \n",
    "    \n",
    "    return {'defaulters_df_final': defaulters_df_final,\n",
    "            'defaulters_df_raw': defaulters_df_raw,\n",
    "            'inactive_drivers_df': splitted_dfs['inactive_drivers_df'],\n",
    "            'not_departed_drivers_df': splitted_dfs['not_departed_drivers_df'],\n",
    "            'behinders_df': splitted_dfs['behinders_df'],\n",
    "            'deliveries_df': splitted_dfs['deliveries_df']}\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    This part is totaly work with private library\n",
    "    \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8f8484-ee6f-4017-a046-b875db38eaad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "808cba46-145d-47c2-8f41-2ea4cd2fd910",
   "metadata": {},
   "source": [
    "#### **Script running code** ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8023c9c-f2c0-4e54-82b8-d63006a09139",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    load_dotenv()\n",
    "    data_folderpath = toolkit.create_folder(projectname=PROJECT_NAME, foldername='data')\n",
    "    final_output_fname = f'{datetime.now(config.tz).strftime(\"%Y-%m-%d_AmazonDefaulters_%HHrs\")}.xlsx'\n",
    "    final_output_fpath = data_folderpath + '/' + final_output_fname\n",
    "    t1 = time.time()  # execution start time for the python script\n",
    "\n",
    "    google_creds_fpath = f'/home/ubuntu/atom/{PROJECT_NAME}/google_account_credentials.json' if config.ON_SERVER \\\n",
    "        else f'/Users/Admin/PycharmProjects/{PROJECT_NAME}/google_account_credentials.json'\n",
    "    \"\"\"\n",
    "    This part of is setup the script running environment for server help of dotenv,\n",
    "    and based on server business requirements.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    \n",
    "    try:\n",
    "        print('\\n--------------------***--------------------\\n')\n",
    "        print(f'Execution Started at: {datetime.now(config.tz).strftime(\"%Y-%m-%d %H:%M:%S\")}')\n",
    "\n",
    "        # Retrieve current drivers' data\n",
    "        final_dfs = get_current_drivers_data(sample=SAMPLE)\n",
    "\n",
    "        with pd.ExcelWriter(final_output_fpath, engine=None) as writer:\n",
    "            final_dfs['inactive_drivers_df'].to_excel(writer, sheet_name='Inactive', index=False)\n",
    "            final_dfs['not_departed_drivers_df'].to_excel(writer, sheet_name='Not Departed', index=False)\n",
    "            final_dfs['behinders_df'].to_excel(writer, sheet_name='Behinders', index=False)\n",
    "            final_dfs['deliveries_df'].to_excel(writer, sheet_name='Packages', index=False)\n",
    "            \n",
    "            \"\"\"\n",
    "            This part of code write excel files with data frames.\n",
    "            \"\"\"\n",
    "\n",
    "            \n",
    "            \n",
    "        \n",
    "        s3_foldername = 'whiz-amz-cortex-live'\n",
    "        s3_storage = s3c.connect_to_s3_storage(os.getenv('AWS_ACCESS_KEY_ID'), os.getenv('AWS_SECRET_ACCESS_KEY'))\n",
    "        atom_bucket = s3_storage.Bucket('atom-s3')  # Selecting a bucket from the S3 storage\n",
    "\n",
    "        if not SAMPLE and UPLOAD_TO_S3:\n",
    "            s3c.upload_to_s3(atom_bucket, s3_foldername, final_output_fpath, final_output_fname)\n",
    "\n",
    "        # Get the latest 40 packages history data from S3\n",
    "        req_excel_files = sorted(s3c.get_all_excels(connected_bucket=atom_bucket, folder_name=s3_foldername),\n",
    "                                 reverse=True)[:40]\n",
    "        history_pkgs_df = s3c.concat_excel_sheets_to_df(connected_bucket=atom_bucket,\n",
    "                                                        excel_files=req_excel_files,\n",
    "                                                        sheet_num='Packages')\n",
    "        \n",
    "        \"\"\"\n",
    "        This part of the code setup amazon S3 bucket to store data,\n",
    "        and also ubload requirement data.\n",
    "        This library documention is also private.\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        if UPDATE_GSHEET:\n",
    "            gc.upload_df_to_gsheets(creds_fpath=google_creds_fpath,\n",
    "                                    dataframe=final_dfs['inactive_drivers_df'],\n",
    "                                    file_name=\"amazon-cortex-live-data\",\n",
    "                                    sheet_name=\"Inactive\", formatting=False)\n",
    "            gc.upload_df_to_gsheets(creds_fpath=google_creds_fpath,\n",
    "                                    dataframe=final_dfs['not_departed_drivers_df'],\n",
    "                                    file_name=\"amazon-cortex-live-data\",\n",
    "                                    sheet_name=\"Not Departed\", formatting=False)\n",
    "            gc.upload_df_to_gsheets(creds_fpath=google_creds_fpath,\n",
    "                                    dataframe=final_dfs['behinders_df'],\n",
    "                                    file_name=\"amazon-cortex-live-data\",\n",
    "                                    sheet_name=\"Behinders\", formatting=False)\n",
    "            gc.upload_df_to_gsheets(creds_fpath=google_creds_fpath,\n",
    "                                    dataframe=final_dfs['deliveries_df'],\n",
    "                                    file_name=\"amazon-cortex-live-data\",\n",
    "                                    sheet_name=\"Packages\", formatting=False)\n",
    "            gc.upload_df_to_gsheets(creds_fpath=google_creds_fpath,\n",
    "                                    dataframe=history_pkgs_df,\n",
    "                                    file_name=\"amazon-cortex-live-data\",\n",
    "                                    sheet_name=\"History - Packages\", formatting=False)\n",
    "            \"\"\"\n",
    "            This part is upload data frame to a goole sheet,\n",
    "            the library documention is private.\n",
    "            \"\"\"\n",
    "            \n",
    "            \n",
    "        \n",
    "        \n",
    "        print(f'Total Time for the Complete Execution: {(time.time() - t1) / 60:.3f} minutes')\n",
    "        print(f'Execution Completed at: {datetime.now(config.tz).strftime(\"%Y-%m-%d %H:%M:%S\")}')\n",
    "        \"\"\"\n",
    "        This part of code printing total time of execution of the script,\n",
    "        and also print execution server time.\n",
    "        \n",
    "        This part is for monitor the script.\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        \n",
    "    except Exception as e:\n",
    "        \"\"\"\n",
    "        This part of code handle error,\n",
    "        if there is any error it will notify developer via emails for recheck. \n",
    "        \"\"\"\n",
    "        message = f'Error occurred while generating the report!\\nError: {type(e).__name__}, {e}'\n",
    "        print(message)\n",
    "        subj = f'Failed: {final_output_fname.replace(\".csv\", \"\")}'\n",
    "        toolkit.send_failure_email(send=SEND_FAIL_EMAIL, from_email=os.getenv('EMAIL_ID'),\n",
    "                                   pwd=os.getenv('EMAIL_PASSWORD'),\n",
    "                                   receiver_email='lafir.malim@whizzard.in',\n",
    "                                   email_subject=subj, email_message=message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af844a7-6a4b-45d9-aee6-17501b43272d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f12775e-aa2e-42e4-a0e2-93982f84e155",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e8503573-3835-40f3-b45d-c9be8f5ce9f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>site_code</th>\n",
       "      <th>service_area_id</th>\n",
       "      <th>region_id</th>\n",
       "      <th>region_name</th>\n",
       "      <th>state</th>\n",
       "      <th>postal_code</th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>active</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DAK1</td>\n",
       "      <td>ba419f3f-d223-44d4-83fc-a87212a83fbe</td>\n",
       "      <td>19920c05-6bb1-429f-b995-444e3b8bbb61</td>\n",
       "      <td>Abha</td>\n",
       "      <td>Abha</td>\n",
       "      <td>20642</td>\n",
       "      <td>42.649750</td>\n",
       "      <td>18.254000</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AGRD</td>\n",
       "      <td>b8e4652b-cea4-4e1b-b61f-902a4e687b21</td>\n",
       "      <td>39f2ef6a-4294-4066-b5d7-86dc9ac811e5</td>\n",
       "      <td>Agra</td>\n",
       "      <td>UP</td>\n",
       "      <td>282006</td>\n",
       "      <td>78.062374</td>\n",
       "      <td>27.212077</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FUPD</td>\n",
       "      <td>8e97362b-0951-406d-b2ea-9aa1fef9660e</td>\n",
       "      <td>39f2ef6a-4294-4066-b5d7-86dc9ac811e5</td>\n",
       "      <td>Agra</td>\n",
       "      <td>UTTAR PRADESH</td>\n",
       "      <td>282001</td>\n",
       "      <td>82.923594</td>\n",
       "      <td>25.286959</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PNAM</td>\n",
       "      <td>ac3aa2bc-98d2-4ce6-9b36-c8b88161ba95</td>\n",
       "      <td>39f2ef6a-4294-4066-b5d7-86dc9ac811e5</td>\n",
       "      <td>Agra</td>\n",
       "      <td>UTTAR PRADESH</td>\n",
       "      <td>282007</td>\n",
       "      <td>78.053900</td>\n",
       "      <td>27.148700</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>UPAA</td>\n",
       "      <td>49c0a838-e0ea-4899-84d5-829a9eb87bdb</td>\n",
       "      <td>39f2ef6a-4294-4066-b5d7-86dc9ac811e5</td>\n",
       "      <td>Agra</td>\n",
       "      <td>Uttar Pradesh</td>\n",
       "      <td>282007</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  site_code                       service_area_id  \\\n",
       "0      DAK1  ba419f3f-d223-44d4-83fc-a87212a83fbe   \n",
       "1      AGRD  b8e4652b-cea4-4e1b-b61f-902a4e687b21   \n",
       "1      FUPD  8e97362b-0951-406d-b2ea-9aa1fef9660e   \n",
       "1      PNAM  ac3aa2bc-98d2-4ce6-9b36-c8b88161ba95   \n",
       "1      UPAA  49c0a838-e0ea-4899-84d5-829a9eb87bdb   \n",
       "\n",
       "                              region_id region_name          state  \\\n",
       "0  19920c05-6bb1-429f-b995-444e3b8bbb61        Abha           Abha   \n",
       "1  39f2ef6a-4294-4066-b5d7-86dc9ac811e5        Agra             UP   \n",
       "1  39f2ef6a-4294-4066-b5d7-86dc9ac811e5        Agra  UTTAR PRADESH   \n",
       "1  39f2ef6a-4294-4066-b5d7-86dc9ac811e5        Agra  UTTAR PRADESH   \n",
       "1  39f2ef6a-4294-4066-b5d7-86dc9ac811e5        Agra  Uttar Pradesh   \n",
       "\n",
       "  postal_code  longitude   latitude  active  \n",
       "0       20642  42.649750  18.254000    True  \n",
       "1      282006  78.062374  27.212077    True  \n",
       "1      282001  82.923594  25.286959    True  \n",
       "1      282007  78.053900  27.148700    True  \n",
       "1      282007   0.000000   0.000000    True  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "\"\"\"\n",
    "This part of library are Python inbuild.\n",
    "- os Library for setup path for server\n",
    "- json Libray for converting API data to Python object\n",
    "- datetime and 'timedelta' for converting date object to Python object and 'datetime' is for geting current datetime\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "# noinspection SpellCheckingInspection  (Function 1)\n",
    "def get_service_area_id():\n",
    "    \"\"\"\n",
    "    Retrieves service area IDs from an API endpoint.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing service area information.\n",
    "    \"\"\"\n",
    "\n",
    "    payload = \"\"\n",
    "    headers = {\"cookie\": \"session-id=260-6428466-7223316; session-id-time=2082787201l; \"\n",
    "                         \"session-token=GbElPtRlx37WtbcOToTC3cRl2schxCQbGfw9PermyrbyTyonOdmMyDvN9xAn3Y1u\"\n",
    "                         \"urDFHnNpykoR0H9l8R%2Bw0pqKOD14Zwtk4QClUVQpQsIItI6ETeBDN0M5SpFCUku0BGvpR2xSOFJN\"\n",
    "                         \"nMUMDfYiflLntxjcLhKYtat1HekaJ2LZEwTlJ3dL6%2Fs%2B9U37Zu8JIQq%2FK7qPG5HFr4SwvqwIoQ\"\n",
    "                         \"pyvIMoFI7ZbtIDa1gNSij8nXYZmqmACRQ0FUVbIFHYlcHjlwmkXzg%3D\"}\n",
    "\n",
    "    url = \"https://logistics.amazon.in/flex/api/getOperationalRegions\"\n",
    "    response = requests.get(url, data=payload, headers=headers)\n",
    "\n",
    "    data = json.loads(response.text)\n",
    "    req_data = data\n",
    "    api_df_raw = pd.DataFrame(req_data)\n",
    "    \n",
    "    \"\"\"\n",
    "    Payload, Header, and URl is sending request materials.\n",
    "    Header conferm, we get write data based on our requirements and URL is parrent path.\n",
    "    Request library under the hood make this into url raw string, that server can understand.\n",
    "    \n",
    "    We converted APIs json response to Python json object help of json.loads()\n",
    "    We converted json object to pandas data frame 'api_df_raw'\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "\n",
    "    # Explode the 'basicServiceAreas' column into multiple rows\n",
    "    api_df = api_df_raw.explode('basicServiceAreas')\n",
    "\n",
    "    # Split the exploded column into separate columns\n",
    "    api_df_inter = pd.concat(\n",
    "        [\n",
    "            api_df.drop(['basicServiceAreas'], axis=1),\n",
    "            api_df['basicServiceAreas'].apply(pd.Series)\n",
    "        ],\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    # Concatenate the necessary columns for the final DataFrame\n",
    "    df_final = pd.concat(\n",
    "        [\n",
    "            api_df_inter.drop(['pickUpLocationAddress', 'pickUpLocation'], axis=1),\n",
    "            api_df_inter['pickUpLocationAddress'].apply(pd.Series),\n",
    "            api_df_inter['pickUpLocation'].apply(pd.Series)\n",
    "        ],\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    # Select the desired columns and rename them\n",
    "    final_df = df_final[['defaultStationCode', 'serviceAreaID', 'regionID', 'regionName',\n",
    "                         'state', 'postalCode', 'longitude', 'latitude', 'active']]\n",
    "    final_df.columns = ['site_code', 'service_area_id', 'region_id', 'region_name',\n",
    "                        'state', 'postal_code', 'longitude', 'latitude', 'active']\n",
    "    \n",
    "    \"\"\"\n",
    "    This part doing data manupulation task based on business requirements, and return a final clean data frame\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    return final_df\n",
    "\n",
    "df = get_service_area_id()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ea6cd8-950b-44f3-94c2-fae5ea65de03",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7c23a6-e530-4128-b0ea-2e8f9a0427da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
